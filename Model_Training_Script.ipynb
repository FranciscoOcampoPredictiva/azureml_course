{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoOcampoPredictiva/azureml_course/blob/main/Model_Training_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the class\n",
        "from azureml.core import Run"
      ],
      "metadata": {
        "id": "CGtn2iG5nBkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the run context\n",
        "new_run = Run.get_context()"
      ],
      "metadata": {
        "id": "tyGNix3WngLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the workspace\n",
        "ws = new_run.experiment.workspace"
      ],
      "metadata": {
        "id": "bujA0hzvnhvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the arguments from the pipeline job\n",
        "from argparse import ArgumentParser as AP\n",
        "parser = AP()\n",
        "parser.add_argument('--datafolder', type=str) # Adding the argument\n",
        "args = parser.parse_args()  # Passing the arguments in args"
      ],
      "metadata": {
        "id": "D5nXK9olni5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data from previous step\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Create the path\n",
        "path = os.path.join(args.datafolder, 'churn_prep.csv')\n",
        "dataPrep = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "8ti6rzpFnqM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and Y\n",
        "X = dataPrep.drop(['Exited'], axis=1)\n",
        "Y = dataPrep[['Exited']]"
      ],
      "metadata": {
        "id": "ktKW8vVPoEy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0, stratify=Y)"
      ],
      "metadata": {
        "id": "62umwcxnoVop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train the Logistic Regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "_igy3CxAoVr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the output - Scored Label\n",
        "Y_predict = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "6bgnnrwzoVvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scored Probabilities\n",
        "Y_prob = classifier.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "KWiE4Hlbomzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix and accuracy score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm    = confusion_matrix(Y_test, Y_predict)\n",
        "score = classifier.score(X_test, Y_test)"
      ],
      "metadata": {
        "id": "cg5QuxkFom3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log metrics\n",
        "\n",
        "# Create the confusion matrix dictionary\n",
        "cm_dict = {\"schema_type\": \"confusion_matrix\",\n",
        "           \"schema_version\": \"v1\",\n",
        "           \"data\": {\"class_labels\": [\"N\", \"Y\"],\n",
        "                    \"matrix\": cm.tolist()}\n",
        "           }\n",
        "\n",
        "new_run.log('TotalObservations', len(dataPrep))\n",
        "new_run.log_confusion_matrix('ConfusionMatrix', cm_dict)\n",
        "new_run.log('Score', score)"
      ],
      "metadata": {
        "id": "ZlZMpbj7opSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Scored Dataset and upload to outputs folder\n",
        "\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "Y_test = Y_test.reset_index(drop=True)\n",
        "\n",
        "Y_prob_df    = pd.DataFrame(Y_prob, columns=['Scored Probabilities'])\n",
        "Y_predict_df = pd.DataFrame(Y_predict, columns=['Scored Label'])\n",
        "\n",
        "scored_dataset = pd.concat([X_test, Y_test, Y_predict_df, Y_prob_df], axis=1)"
      ],
      "metadata": {
        "id": "-VJLZJOyopWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the scored dataset\n",
        "scored_dataset.to_csv('./outputs/churn_scored.csv', index=False)"
      ],
      "metadata": {
        "id": "0wL2GZfhor4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the run\n",
        "new_run.complete()"
      ],
      "metadata": {
        "id": "O_tHB6ltotGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8k2J_dlTotQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}